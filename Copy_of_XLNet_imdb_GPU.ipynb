{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of XLNet-imdb-GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eriche523/eriche523/blob/master/Copy_of_XLNet_imdb_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fnOHnctkG6kW"
      },
      "source": [
        "# XLNet IMDB movie review classification project\n",
        "\n",
        "This notebook is for classifying the [imdb sentiment dataset](https://ai.stanford.edu/~amaas/data/sentiment/).  It will be easy to edit this notebook in order to run all of the classification tasks referenced in the [XLNet paper](https://arxiv.org/abs/1906.08237). Whilst you cannot expect to obtain the state-of-the-art results in the paper on a GPU, this model will still score very highly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2mBzLdrdzodb"
      },
      "source": [
        "## Setup\n",
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hRHRPImGUth7",
        "outputId": "2d057a8e-8593-4065-ad47-19cead2e2e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "! pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jy8gUsPuJNyw"
      },
      "source": [
        "Download the pretrained XLNet model and unzip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HfPDGsUtHKG0",
        "outputId": "0c005c6c-84d0-4586-d3c9-8de3e4d01c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "# only needs to be done once\n",
        "! wget https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n",
        "! unzip cased_L-24_H-1024_A-16.zip "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-20 07:13:21--  https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.120.128, 2607:f8b0:4001:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.120.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1338042341 (1.2G) [application/zip]\n",
            "Saving to: ‘cased_L-24_H-1024_A-16.zip’\n",
            "\n",
            "cased_L-24_H-1024_A 100%[===================>]   1.25G   155MB/s    in 8.9s    \n",
            "\n",
            "2020-01-20 07:13:30 (144 MB/s) - ‘cased_L-24_H-1024_A-16.zip’ saved [1338042341/1338042341]\n",
            "\n",
            "Archive:  cased_L-24_H-1024_A-16.zip\n",
            "   creating: xlnet_cased_L-24_H-1024_A-16/\n",
            "  inflating: xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt.index  \n",
            "  inflating: xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt.data-00000-of-00001  \n",
            "  inflating: xlnet_cased_L-24_H-1024_A-16/spiece.model  \n",
            "  inflating: xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt.meta  \n",
            "  inflating: xlnet_cased_L-24_H-1024_A-16/xlnet_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4uUwjq3BJRbu"
      },
      "source": [
        "Download extract the imdb dataset - surpessing output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QOGRICbOIsU8",
        "outputId": "4e60b3a0-49fb-4bbd-8f46-eddb5f7707c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar zxf aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-20 07:14:01--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  39.3MB/s    in 2.0s    \n",
            "\n",
            "2020-01-20 07:14:03 (39.3 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yGY_ggUUMrwU"
      },
      "source": [
        "Git clone XLNet repo for access to run_classifier and the rest of the xlnet module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-r190eYVMpiG",
        "outputId": "589a9071-92cd-475c-a0a1-1dc333a99530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "! git clone https://github.com/zihangdai/xlnet.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'xlnet'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Total 122 (delta 0), reused 0 (delta 0), pack-reused 122\u001b[K\n",
            "Receiving objects: 100% (122/122), 2.92 MiB | 16.80 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jDP-IaVuPC-z"
      },
      "source": [
        "## Define Variables\n",
        "Define all the dirs: data, xlnet scripts & pretrained model. \n",
        "If you would like to save models then you can authenticate a GCP account and use that for the OUTPUT_DIR & CHECKPOINT_DIR - you will need a large amount storage to fix these models. \n",
        "\n",
        "Alternatively it is easy to integrate a google drive account, checkout this guide for [I/O in colab](https://colab.research.google.com/notebooks/io.ipynb) but rememeber these will take up a large amount of storage. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y7N_xVwavQlV",
        "colab": {}
      },
      "source": [
        "SCRIPTS_DIR = 'xlnet' #@param {type:\"string\"}\n",
        "DATA_DIR = 'aclImdb' #@param {type:\"string\"}\n",
        "OUTPUT_DIR = 'proc_data/imdb' #@param {type:\"string\"}\n",
        "PRETRAINED_MODEL_DIR = 'xlnet_cased_L-24_H-1024_A-16' #@param {type:\"string\"}\n",
        "CHECKPOINT_DIR = 'exp/imdb' #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jR6euqwL1KBV"
      },
      "source": [
        "## Run Model\n",
        "This will set off the fine tuning of XLNet. There are a few things to note here:\n",
        "\n",
        "\n",
        "1.   This script will train and evaluate the model\n",
        "2.   This will store the results locally on colab and will be lost when you are disconnected from the runtime\n",
        "3.   This uses the large version of the model (base not released presently)\n",
        "4.   We are using a max seq length of 128 with a batch size of 8 please refer to the [README](https://github.com/zihangdai/xlnet#memory-issue-during-finetuning) for why this is.\n",
        "5. This will take approx 4hrs to run on GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CEMuT6LU0avg",
        "outputId": "ec041aeb-1776-40f9-e63b-f24290452aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_command = \"python xlnet/run_classifier.py \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --eval_all_ckpt=True \\\n",
        "  --task_name=imdb \\\n",
        "  --data_dir=\"+DATA_DIR+\" \\\n",
        "  --output_dir=\"+OUTPUT_DIR+\" \\\n",
        "  --model_dir=\"+CHECKPOINT_DIR+\" \\\n",
        "  --uncased=False \\\n",
        "  --spiece_model_file=\"+PRETRAINED_MODEL_DIR+\"/spiece.model \\\n",
        "  --model_config_path=\"+PRETRAINED_MODEL_DIR+\"/xlnet_config.json \\\n",
        "  --init_checkpoint=\"+PRETRAINED_MODEL_DIR+\"/xlnet_model.ckpt \\\n",
        "  --max_seq_length=128 \\\n",
        "  --train_batch_size=8 \\\n",
        "  --eval_batch_size=8 \\\n",
        "  --num_hosts=1 \\\n",
        "  --num_core_per_host=1 \\\n",
        "  --learning_rate=2e-5 \\\n",
        "  --train_steps=4000 \\\n",
        "  --warmup_steps=500 \\\n",
        "  --save_steps=1000 \\\n",
        "  --iterations=500\"\n",
        "\n",
        "! {train_command}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/xlnet/model_utils.py:295: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:855: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:637: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0120 07:14:49.443495 140481317656448 module_wrapper.py:139] From xlnet/run_classifier.py:637: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:637: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0120 07:14:49.443719 140481317656448 module_wrapper.py:139] From xlnet/run_classifier.py:637: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:661: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "W0120 07:14:49.444218 140481317656448 module_wrapper.py:139] From xlnet/run_classifier.py:661: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:662: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0120 07:14:49.444551 140481317656448 module_wrapper.py:139] From xlnet/run_classifier.py:662: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/xlnet/model_utils.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0120 07:14:49.499248 140481317656448 module_wrapper.py:139] From /content/xlnet/model_utils.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/xlnet/model_utils.py:36: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0120 07:14:49.499558 140481317656448 module_wrapper.py:139] From /content/xlnet/model_utils.py:36: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Single device mode.\n",
            "I0120 07:14:49.499673 140481317656448 model_utils.py:36] Single device mode.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0120 07:14:49.499805 140481317656448 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "I0120 07:14:50.400669 140481317656448 utils.py:141] NumExpr defaulting to 2 threads.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'exp/imdb', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            ", '_keep_checkpoint_max': 0, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc409dd2a20>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=1, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
            "I0120 07:14:51.626319 140481317656448 estimator.py:212] Using config: {'_model_dir': 'exp/imdb', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            ", '_keep_checkpoint_max': 0, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc409dd2a20>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=1, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function get_model_fn.<locals>.model_fn at 0x7fc409dd9268>) includes params argument, but params are not passed to Estimator.\n",
            "W0120 07:14:51.627139 140481317656448 model_fn.py:630] Estimator's model_fn (<function get_model_fn.<locals>.model_fn at 0x7fc409dd9268>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Use tfrecord file proc_data/imdb/spiece.model.len-128.train.tf_record\n",
            "I0120 07:14:51.627677 140481317656448 run_classifier.py:703] Use tfrecord file proc_data/imdb/spiece.model.len-128.train.tf_record\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:312: The name tf.gfile.ListDirectory is deprecated. Please use tf.io.gfile.listdir instead.\n",
            "\n",
            "W0120 07:14:51.627834 140481317656448 module_wrapper.py:139] From xlnet/run_classifier.py:312: The name tf.gfile.ListDirectory is deprecated. Please use tf.io.gfile.listdir instead.\n",
            "\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:316: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0120 07:14:51.643296 140481317656448 module_wrapper.py:139] From xlnet/run_classifier.py:316: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "INFO:tensorflow:Num of train samples: 25000\n",
            "I0120 07:14:55.603867 140481317656448 run_classifier.py:707] Num of train samples: 25000\n",
            "INFO:tensorflow:Create new tfrecord proc_data/imdb/spiece.model.len-128.train.tf_record.\n",
            "I0120 07:14:55.604273 140481317656448 run_classifier.py:405] Create new tfrecord proc_data/imdb/spiece.model.len-128.train.tf_record.\n",
            "WARNING:tensorflow:From xlnet/run_classifier.py:407: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0120 07:14:55.604569 140481317656448 module_wrapper.py:139] From xlnet/run_classifier.py:407: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "INFO:tensorflow:Writing example 0 of 25000\n",
            "I0120 07:14:55.604810 140481317656448 run_classifier.py:415] Writing example 0 of 25000\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0120 07:14:55.606803 140481317656448 classifier_utils.py:133] *** Example ***\n",
            "INFO:tensorflow:guid: unused_id\n",
            "I0120 07:14:55.606925 140481317656448 classifier_utils.py:134] guid: unused_id\n",
            "INFO:tensorflow:input_ids: 1977 292 13565 1399 4377 45 24955 2059 22 18 1432 2268 25 18 468 1631 26403 9 67 52 468 1399 2254 18473 150 19 48 3281 93 40 24 2653 4557 367 61 8683 22 10601 18 8192 29 18 367 54 572 308 40 103 9 324 112 52 18473 150 9488 22 401 24 18681 25 374 22 633 24 1791 21 4891 235 19 57 5899 3738 29 43 2146 76 708 40 45 396 134 43 64 786 22 2304 784 9 1533 340 20 18 2653 4557 367 54 7028 38 18 219 92 34 18473 150 19 57 63 210 22 133 176 33 18 185 870 100 278 5519 784 9 4255 18473 150 2768 18 114 4 3\n",
            "I0120 07:14:55.607094 140481317656448 classifier_utils.py:135] input_ids: 1977 292 13565 1399 4377 45 24955 2059 22 18 1432 2268 25 18 468 1631 26403 9 67 52 468 1399 2254 18473 150 19 48 3281 93 40 24 2653 4557 367 61 8683 22 10601 18 8192 29 18 367 54 572 308 40 103 9 324 112 52 18473 150 9488 22 401 24 18681 25 374 22 633 24 1791 21 4891 235 19 57 5899 3738 29 43 2146 76 708 40 45 396 134 43 64 786 22 2304 784 9 1533 340 20 18 2653 4557 367 54 7028 38 18 219 92 34 18473 150 19 57 63 210 22 133 176 33 18 185 870 100 278 5519 784 9 4255 18473 150 2768 18 114 4 3\n",
            "INFO:tensorflow:input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0120 07:14:55.607270 140481317656448 classifier_utils.py:136] input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "I0120 07:14:55.607404 140481317656448 classifier_utils.py:137] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "INFO:tensorflow:label: pos (id = 1)\n",
            "I0120 07:14:55.607486 140481317656448 classifier_utils.py:138] label: pos (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0120 07:14:55.608550 140481317656448 classifier_utils.py:133] *** Example ***\n",
            "INFO:tensorflow:guid: unused_id\n",
            "I0120 07:14:55.608717 140481317656448 classifier_utils.py:134] guid: unused_id\n",
            "INFO:tensorflow:input_ids: 122 1432 30 18 252 1432 35 47 545 566 9 7981 657 5915 35 1790 4412 52 1432 149 44 41 551 22 567 24 70 2172 75 18 235 20 2717 1690 9 1177 18 1432 30 50 140 33 1790 15398 6454 36 27 24 7459 21 235 3110 1432 29 64 39 4163 21 11244 37 1251 9 35 685 52 1432 33 94 273 21 35 64 4151 4936 29 80 47 71 54 24 459 20 758 9 122 1432 1765 104 22 343 1111 160 500 18 235 30 28 18 12709 21 160 178 17 15869 7250 43 30 3283 33 9 320 35 685 52 1432 19 7602 30 50 24 570 2775 1715 25 18 1253 4 3\n",
            "I0120 07:14:55.608861 140481317656448 classifier_utils.py:135] input_ids: 122 1432 30 18 252 1432 35 47 545 566 9 7981 657 5915 35 1790 4412 52 1432 149 44 41 551 22 567 24 70 2172 75 18 235 20 2717 1690 9 1177 18 1432 30 50 140 33 1790 15398 6454 36 27 24 7459 21 235 3110 1432 29 64 39 4163 21 11244 37 1251 9 35 685 52 1432 33 94 273 21 35 64 4151 4936 29 80 47 71 54 24 459 20 758 9 122 1432 1765 104 22 343 1111 160 500 18 235 30 28 18 12709 21 160 178 17 15869 7250 43 30 3283 33 9 320 35 685 52 1432 19 7602 30 50 24 570 2775 1715 25 18 1253 4 3\n",
            "INFO:tensorflow:input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0120 07:14:55.608985 140481317656448 classifier_utils.py:136] input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "I0120 07:14:55.609087 140481317656448 classifier_utils.py:137] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "INFO:tensorflow:label: pos (id = 1)\n",
            "I0120 07:14:55.609164 140481317656448 classifier_utils.py:138] label: pos (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0120 07:14:55.610659 140481317656448 classifier_utils.py:133] *** Example ***\n",
            "INFO:tensorflow:guid: unused_id\n",
            "I0120 07:14:55.610792 140481317656448 classifier_utils.py:134] guid: unused_id\n",
            "INFO:tensorflow:input_ids: 35 6024 19 491 280 20 679 25 18 5514 15503 13 1528 4497 117 679 19 35 569 24 17 23 18774 28 166 713 20 3547 9 35 64 1991 106 20 18 17 2800 4209 20 18 5514 15503 9428 420 70 100 127 17 12 17003 12 104 9 476 19 52 1432 27 1172 10398 25 300 2147 1138 5510 386 162 9 3269 17 46 9139 9974 20 18 5514 15503 17 9785 13152 27 2037 34 13785 19 17027 2201 24 673 206 1162 90 43 17104 7515 24 2352 9 122 1432 27 102 11601 29 36 27 50 176 5787 22 1628 34 24 216 1168 31 18 6662 202 20 18 5514 15503 18878 9 84 4 3\n",
            "I0120 07:14:55.610903 140481317656448 classifier_utils.py:135] input_ids: 35 6024 19 491 280 20 679 25 18 5514 15503 13 1528 4497 117 679 19 35 569 24 17 23 18774 28 166 713 20 3547 9 35 64 1991 106 20 18 17 2800 4209 20 18 5514 15503 9428 420 70 100 127 17 12 17003 12 104 9 476 19 52 1432 27 1172 10398 25 300 2147 1138 5510 386 162 9 3269 17 46 9139 9974 20 18 5514 15503 17 9785 13152 27 2037 34 13785 19 17027 2201 24 673 206 1162 90 43 17104 7515 24 2352 9 122 1432 27 102 11601 29 36 27 50 176 5787 22 1628 34 24 216 1168 31 18 6662 202 20 18 5514 15503 18878 9 84 4 3\n",
            "INFO:tensorflow:input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0120 07:14:55.611012 140481317656448 classifier_utils.py:136] input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "I0120 07:14:55.611114 140481317656448 classifier_utils.py:137] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "INFO:tensorflow:label: neg (id = 0)\n",
            "I0120 07:14:55.611209 140481317656448 classifier_utils.py:138] label: neg (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0120 07:14:55.612585 140481317656448 classifier_utils.py:133] *** Example ***\n",
            "INFO:tensorflow:guid: unused_id\n",
            "I0120 07:14:55.612698 140481317656448 classifier_utils.py:134] guid: unused_id\n",
            "INFO:tensorflow:input_ids: 122 1432 30 24 1009 3419 20 92 9 35 5607 52 1432 33 24 256 20 28186 18449 19 61 47 9958 3736 25 3547 655 426 24 8837 1715 28 24 154 20 17 12 2418 3867 12 17 10 712 65 20 94 2533 1709 270 31 52 335 54 18 17 12353 22 3772 52 4990 56 8002 17 11 97 50 65 20 107 255 24 570 18071 56 882 9 169 1348 18 1432 33 24 17 7948 56 14486 20 2002 4518 7578 23 35 53 50 27808 193 9 35 64 114 3132 29 18 1654 3312 915 193 55 1062 37 3076 5580 13453 19 666 9 320 2225 19 34 65 20 186 16010 23 53 4 3\n",
            "I0120 07:14:55.612804 140481317656448 classifier_utils.py:135] input_ids: 122 1432 30 24 1009 3419 20 92 9 35 5607 52 1432 33 24 256 20 28186 18449 19 61 47 9958 3736 25 3547 655 426 24 8837 1715 28 24 154 20 17 12 2418 3867 12 17 10 712 65 20 94 2533 1709 270 31 52 335 54 18 17 12353 22 3772 52 4990 56 8002 17 11 97 50 65 20 107 255 24 570 18071 56 882 9 169 1348 18 1432 33 24 17 7948 56 14486 20 2002 4518 7578 23 35 53 50 27808 193 9 35 64 114 3132 29 18 1654 3312 915 193 55 1062 37 3076 5580 13453 19 666 9 320 2225 19 34 65 20 186 16010 23 53 4 3\n",
            "INFO:tensorflow:input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0120 07:14:55.612909 140481317656448 classifier_utils.py:136] input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "I0120 07:14:55.613068 140481317656448 classifier_utils.py:137] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "INFO:tensorflow:label: neg (id = 0)\n",
            "I0120 07:14:55.613165 140481317656448 classifier_utils.py:138] label: neg (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0120 07:14:55.614168 140481317656448 classifier_utils.py:133] *** Example ***\n",
            "INFO:tensorflow:guid: unused_id\n",
            "I0120 07:14:55.614368 140481317656448 classifier_utils.py:134] guid: unused_id\n",
            "INFO:tensorflow:input_ids: 1641 2242 18 28964 31 52 468 28 87 20533 19 5306 35 3419 94 356 136 14239 19 35 9856 4168 17 12 5941 46 249 12 585 21 20479 17 12 753 902 9245 26 23 1869 12 15602 19 57 24717 136 122 393 724 845 27 24 17 88 409 4909 20 24 3578 86 27773 2701 97 27505 66 7353 20 27773 23 9715 23 3411 91 24 17 409 4287 584 19 1469 76 5165 21 931 19 21 137 41 2917 25 18 326 37 24 14618 56 6609 9 4168 585 1898 18 188 34 24 6609 19 57 43 26 23 2279 3587 93 19 21 45 5041 249 5646 9 13917 19 125 1601 26 46 4 3\n",
            "I0120 07:14:55.614500 140481317656448 classifier_utils.py:135] input_ids: 1641 2242 18 28964 31 52 468 28 87 20533 19 5306 35 3419 94 356 136 14239 19 35 9856 4168 17 12 5941 46 249 12 585 21 20479 17 12 753 902 9245 26 23 1869 12 15602 19 57 24717 136 122 393 724 845 27 24 17 88 409 4909 20 24 3578 86 27773 2701 97 27505 66 7353 20 27773 23 9715 23 3411 91 24 17 409 4287 584 19 1469 76 5165 21 931 19 21 137 41 2917 25 18 326 37 24 14618 56 6609 9 4168 585 1898 18 188 34 24 6609 19 57 43 26 23 2279 3587 93 19 21 45 5041 249 5646 9 13917 19 125 1601 26 46 4 3\n",
            "INFO:tensorflow:input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0120 07:14:55.614623 140481317656448 classifier_utils.py:136] input_mask: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "I0120 07:14:55.614726 140481317656448 classifier_utils.py:137] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "INFO:tensorflow:label: neg (id = 0)\n",
            "I0120 07:14:55.614848 140481317656448 classifier_utils.py:138] label: neg (id = 0)\n",
            "INFO:tensorflow:Writing example 10000 of 25000\n",
            "I0120 07:15:08.449451 140481317656448 run_classifier.py:415] Writing example 10000 of 25000\n",
            "INFO:tensorflow:Writing example 20000 of 25000\n",
            "I0120 07:15:21.287926 140481317656448 run_classifier.py:415] Writing example 20000 of 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VvhqD-sO0Kyh"
      },
      "source": [
        "## Running & Results\n",
        "These are the results that I got from running this experiment\n",
        "### Params\n",
        "*    --max_seq_length=128 \\\n",
        "*    --train_batch_size= 8 \n",
        "\n",
        "### Times\n",
        "*   Training: 1hr 11mins\n",
        "*   Evaluation: 2.5hr\n",
        "\n",
        "### Results\n",
        "*  Most accurate model on final step\n",
        "*  Accuracy: 0.92416, eval_loss: 0.31708\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XUW2avFM_fi_"
      },
      "source": [
        "### Model\n",
        "\n",
        "*   The trained model checkpoints can be found in 'exp/imdb'\n",
        "\n"
      ]
    }
  ]
}